{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from librosa.display import specshow\n",
    "import torch\n",
    "from modules import mel2wav_vocoder\n",
    "from models.models_HiFi import Generator as HiFiGAN\n",
    "from modules import AttrDict\n",
    "import torchaudio\n",
    "import os\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **plot_mel 함수**\n",
    "- predict mel과 target mel을 시각화해서 비교해볼 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mel(predict_mel, target_mel, sr=22050, hop_length=256):\n",
    "    \"\"\"\n",
    "    predict mel vs target mel을 시각화해서 볼 수 있는 함수\n",
    "\n",
    "    returns: 시각화 출력\n",
    "    \"\"\"\n",
    "    predict_mel = predict_mel.detach().cpu().numpy()\n",
    "    target_mel = target_mel.detach().cpu().numpy()\n",
    "\n",
    "    fig, axes = plt.subplots(1,2,figsize=(10,5))\n",
    "\n",
    "    # 예측 이미지\n",
    "    predict_img = specshow(predict_mel, x_axis=\"time\", y_axis='mel', sr=sr,\n",
    "                           hop_length=hop_length, ax=axes[0], cmap='viridis')\n",
    "    fig.colorbar(predict_img, ax=axes[0], format=\"%+2.0f DB\")\n",
    "    axes[0].set(title=\"Predicted Mel\")\n",
    "\n",
    "\n",
    "    # 타겟 이미지\n",
    "    target_img = specshow(target_mel,x_axis=\"time\", y_axis='mel', sr=sr,\n",
    "                           hop_length=hop_length, ax=axes[1] cmap='viridis' )\n",
    "    fig.colorbar(predict_img, ax=axes[0], format=\"%+2.0f DB\")\n",
    "    axes[1].set(title=\"Target Mel\")\n",
    "\n",
    "    #plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **generate_wav_vocoder 함수**\n",
    "- vocoder(사전학습완료된) 사용해서 멜 스펙트로그램 -> 음성으로 변환하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wav_vocoder(mel_spectrogram, vocoder_path, output_sr=16000):\n",
    "    \"\"\"\n",
    "    보코더 사용해서 멜 스펙트로그램 -> 음성 변환하는 함수\n",
    "    args:\n",
    "        mel_spectrogram: 멜 스펙트로그램(80,172) 형태가 될 것\n",
    "        vocoder_path: HiFiGAN 사용예정\n",
    "\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # vocoder 설정\n",
    "    vocoder_config_path = os.path.join(os.path.driname(vocoder_path), \"config.json\")\n",
    "    with open(vocoder_config_path, \"r\") as f:\n",
    "        config = json.load(f)\n",
    "    h= AttrDict(config)\n",
    "    vocoder = HiFiGAN(h).to(device)\n",
    "\n",
    "    # pretrained model 가중치 로드\n",
    "    state_dict = torch.load(vocoder_path, map_location = device)\n",
    "    vocoder.load_state_dict(state_dict['generator'])\n",
    "    vocoder.eval()\n",
    "\n",
    "    # 음성으로 변환\n",
    "    audio = mel2wav_vocoder(mel_spectrogram, vocoder, 1)\n",
    "\n",
    "    if output_sr != 22050:\n",
    "        audio = torchaudio.tunctional.resample(audio, original_freq = 22050, new_freq= output_sr)\n",
    "    return audio\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
